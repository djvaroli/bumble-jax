{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from bumblejax.model import TransformerLM\n",
    "from bumblejax.train import CollatorForCausalLM, LMTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomIntDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        seq_len: int,\n",
    "        vocab_size: int, \n",
    "        n_samples: int = 10,\n",
    "        seed: int = 42,\n",
    "        padding_amount: int | None = None\n",
    "    ) -> None:\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.n_samples = n_samples\n",
    "        self.rng = jax.random.PRNGKey(seed)\n",
    "\n",
    "        # could create samples at __getitem__ call instead\n",
    "        self._data = jax.random.randint(\n",
    "            self.rng, (n_samples, seq_len), minval=0, maxval=self.vocab_size\n",
    "        )\n",
    "\n",
    "        self.padding_amount = padding_amount            \n",
    "        if self.padding_amount is not None:\n",
    "            self.pad_token_id = self.vocab_size\n",
    "            self.vocab_size += 1\n",
    "            \n",
    "            padding = jax.numpy.full((n_samples, self.padding_amount), self.pad_token_id)\n",
    "            self._data = jax.numpy.concatenate([self._data, padding], axis=-1)\n",
    "    \n",
    "    def __getitem__(self, index) -> dict[str, list]:\n",
    "        if index > self.n_samples - 1:\n",
    "            raise ValueError(\"Index larger than length.\")\n",
    "        \n",
    "        return {\"input_ids\": self._data[index, :].tolist()}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "class TokenizerStandin:\n",
    "    def __init__(self, pad_token_id: int = 0) -> None:\n",
    "        self.pad_token_id = pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 23:28:37.877162: W external/xla/xla/service/gpu/nvptx_compiler.cc:698] The NVIDIA driver's CUDA version is 11.4 which is older than the ptxas CUDA version (11.8.89). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 1, 0, 4, 4, 4]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = RandomIntDataset(3, 4, padding_amount=3)\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "batch_size = 8\n",
    "\n",
    "rng = torch.Generator()\n",
    "rng.manual_seed(random_seed)\n",
    "\n",
    "collator = CollatorForCausalLM(TokenizerStandin(pad_token_id=train_dataset.pad_token_id))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size,\n",
    "    generator=rng,\n",
    "    collate_fn=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inputs', 'labels', 'lookahead_mask', 'padding_mask', 'special_tokens_mask']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Array([[1, 1, 0, 4, 4, 4],\n",
       "        [2, 1, 0, 4, 4, 4],\n",
       "        [0, 2, 0, 4, 4, 4],\n",
       "        [2, 3, 1, 4, 4, 4],\n",
       "        [0, 0, 1, 4, 4, 4],\n",
       "        [2, 2, 2, 4, 4, 4],\n",
       "        [3, 2, 3, 4, 4, 4],\n",
       "        [2, 1, 2, 4, 4, 4]], dtype=int32),\n",
       " Array([[1, 1, 0, 4, 4, 4],\n",
       "        [2, 1, 0, 4, 4, 4],\n",
       "        [0, 2, 0, 4, 4, 4],\n",
       "        [2, 3, 1, 4, 4, 4],\n",
       "        [0, 0, 1, 4, 4, 4],\n",
       "        [2, 2, 2, 4, 4, 4],\n",
       "        [3, 2, 3, 4, 4, 4],\n",
       "        [2, 1, 2, 4, 4, 4]], dtype=int32),\n",
       " Array([[False, False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True]], dtype=bool))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(list(batch.keys()))\n",
    "batch['inputs'], batch['labels'], batch[\"special_tokens_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerLM(vocab_size=train_dataset.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/orquestra/jax-transformer/optimusjx/model/transformer.py:56: UserWarning: Transformer recieved unknown keyword argument labels - ignoring\n",
      "  warn(f\"Transformer recieved unknown keyword argument {key} - ignoring\")\n",
      "/home/orquestra/jax-transformer/optimusjx/model/transformer.py:56: UserWarning: Transformer recieved unknown keyword argument special_tokens_mask - ignoring\n",
      "  warn(f\"Transformer recieved unknown keyword argument {key} - ignoring\")\n"
     ]
    }
   ],
   "source": [
    "trainer = LMTrainer(\n",
    "    model, \n",
    "    example_batch=batch.copy(), \n",
    "    max_iters=101,\n",
    "    report_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(5, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bumblejax",
   "language": "python",
   "name": "bumblejax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
