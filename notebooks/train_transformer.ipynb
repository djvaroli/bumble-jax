{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import jax\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from optimusjx.model import TransformerLM\n",
    "from optimusjx.train import CollatorForCausalLM, LMTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomIntDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, \n",
    "        seq_len: int,\n",
    "        vocab_size: int, \n",
    "        n_samples: int = 10,\n",
    "        seed: int = 42,\n",
    "        padding_amount: int | None = None\n",
    "    ) -> None:\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.n_samples = n_samples\n",
    "        self.rng = jax.random.PRNGKey(seed)\n",
    "\n",
    "        # could create samples at __getitem__ call instead\n",
    "        self._data = jax.random.randint(\n",
    "            self.rng, (n_samples, seq_len), minval=0, maxval=self.vocab_size\n",
    "        )\n",
    "\n",
    "        self.padding_amount = padding_amount            \n",
    "        if self.padding_amount is not None:\n",
    "            self.pad_token_id = self.vocab_size\n",
    "            self.vocab_size += 1\n",
    "            \n",
    "            padding = jax.numpy.full((n_samples, self.padding_amount), self.pad_token_id)\n",
    "            self._data = jax.numpy.concatenate([self._data, padding], axis=-1)\n",
    "    \n",
    "    def __getitem__(self, index) -> dict[str, list]:\n",
    "        if index > self.n_samples - 1:\n",
    "            raise ValueError(\"Index larger than length.\")\n",
    "        \n",
    "        return {\"input_ids\": self._data[index, :].tolist()}\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "class TokenizerStandin:\n",
    "    def __init__(self, pad_token_id: int = 0) -> None:\n",
    "        self.pad_token_id = pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 23:03:24.727405: W external/xla/xla/service/gpu/nvptx_compiler.cc:698] The NVIDIA driver's CUDA version is 11.4 which is older than the ptxas CUDA version (11.8.89). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 1, 0, 4, 4, 4]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = RandomIntDataset(3, 4, padding_amount=3)\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "batch_size = 2\n",
    "\n",
    "rng = torch.Generator()\n",
    "rng.manual_seed(random_seed)\n",
    "\n",
    "collator = CollatorForCausalLM(TokenizerStandin(pad_token_id=train_dataset.pad_token_id))\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size,\n",
    "    generator=rng,\n",
    "    collate_fn=collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['inputs', 'labels', 'lookahead_mask', 'padding_mask', 'special_tokens_mask']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Array([[1, 1, 0, 4, 4, 4],\n",
       "        [2, 1, 0, 4, 4, 4]], dtype=int32),\n",
       " Array([[1, 1, 0, 4, 4, 4],\n",
       "        [2, 1, 0, 4, 4, 4]], dtype=int32),\n",
       " Array([[False, False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True]], dtype=bool))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(list(batch.keys()))\n",
    "batch['inputs'], batch['labels'], batch[\"special_tokens_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerLM(vocab_size=train_dataset.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/orquestra/jax-transformer/optimusjx/model/transformer.py:56: UserWarning: Transformer recieved unknown keyword argument labels - ignoring\n",
      "  warn(f\"Transformer recieved unknown keyword argument {key} - ignoring\")\n",
      "/home/orquestra/jax-transformer/optimusjx/model/transformer.py:56: UserWarning: Transformer recieved unknown keyword argument special_tokens_mask - ignoring\n",
      "  warn(f\"Transformer recieved unknown keyword argument {key} - ignoring\")\n"
     ]
    }
   ],
   "source": [
    "trainer = LMTrainer(\n",
    "    model, \n",
    "    example_batch=batch.copy(), \n",
    "    max_iters=101,\n",
    "    report_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 / 5:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/5 [00:23<?, ?it/s, loss=1.6438621]\u001b[A\n",
      " 20%|██        | 1/5 [00:23<01:32, 23.18s/it, loss=1.6438621]\u001b[A\n",
      " 20%|██        | 1/5 [00:23<01:32, 23.18s/it, loss=1.5999411]\u001b[A\n",
      " 40%|████      | 2/5 [00:23<01:09, 23.18s/it, loss=1.7082672]\u001b[A\n",
      " 60%|██████    | 3/5 [00:23<00:46, 23.18s/it, loss=1.5150365]\u001b[A\n",
      " 80%|████████  | 4/5 [00:23<00:23, 23.18s/it, loss=1.3049347]\u001b[A\n",
      "Epoch 2 / 5:  20%|██        | 1/5 [00:23<01:33, 23.28s/it, loss=1.55, rng=[2144832284 1361675005]]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s, loss=1.6655241]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:00, 33.31it/s, loss=1.7073815]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:00, 43.09it/s, loss=1.3548726]\u001b[A\n",
      " 60%|██████    | 3/5 [00:00<00:00, 48.26it/s, loss=1.4652253]\u001b[A\n",
      " 80%|████████  | 4/5 [00:00<00:00, 50.91it/s, loss=1.5186874]\u001b[A\n",
      "Epoch 3 / 5:  40%|████      | 2/5 [00:23<01:09, 23.28s/it, loss=1.54, rng=[2397227390 2917881067]]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s, loss=1.095359]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:00, 31.37it/s, loss=1.6451203]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:00, 41.71it/s, loss=1.5125815]\u001b[A\n",
      " 60%|██████    | 3/5 [00:00<00:00, 46.87it/s, loss=1.0237416]\u001b[A\n",
      " 80%|████████  | 4/5 [00:00<00:00, 49.69it/s, loss=1.4938526]\u001b[A\n",
      "Epoch 4 / 5:  60%|██████    | 3/5 [00:23<00:12,  6.10s/it, loss=1.35, rng=[2125600031 3045520928]]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s, loss=1.3435191]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:00, 31.81it/s, loss=1.3601848]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:00, 41.39it/s, loss=1.2748139]\u001b[A\n",
      " 60%|██████    | 3/5 [00:00<00:00, 44.72it/s, loss=1.3835021]\u001b[A\n",
      " 80%|████████  | 4/5 [00:00<00:00, 47.33it/s, loss=1.6396364]\u001b[A\n",
      "Epoch 5 / 5:  80%|████████  | 4/5 [00:23<00:06,  6.10s/it, loss=1.4, rng=[ 461490596 1847085077]] \n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 0/5 [00:00<?, ?it/s, loss=1.1144325]\u001b[A\n",
      " 20%|██        | 1/5 [00:00<00:00, 29.44it/s, loss=1.3876518]\u001b[A\n",
      " 40%|████      | 2/5 [00:00<00:00, 39.49it/s, loss=1.2733614]\u001b[A\n",
      " 60%|██████    | 3/5 [00:00<00:00, 43.50it/s, loss=1.3274997]\u001b[A\n",
      " 80%|████████  | 4/5 [00:00<00:00, 46.15it/s, loss=1.2689729]\u001b[A\n",
      "Epoch 5 / 5: 100%|██████████| 5/5 [00:23<00:00,  4.73s/it, loss=1.27, rng=[2680678292  350530554]]\n"
     ]
    }
   ],
   "source": [
    "trainer.train(5, train_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bumblejax",
   "language": "python",
   "name": "bumblejax"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
